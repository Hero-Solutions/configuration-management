---
- name: Set pipeline datahub_url if not specified
  set_fact:
    datahub_pipeline:
      datahub_url: "{{ datahub_pipeline.datahub_protocol }}://{{ datahub.nginx.server_name }}"
    cacheable: yes
  when: datahub_pipeline.datahub_url is undefined
  become_user: "{{ application_user }}"

- name: Remove trailing slashes from paths
  set_fact:
    datahub_pipeline:
      dir: "{{ datahub_pipeline.dir | regex_replace('\\/$', '') }}"
      fixes:
        repo_dir: "{{ datahub_pipeline.fixes.repo_dir | regex_replace('\\/$', '') }}"
      authority_files:
        repo_dir: "{{ datahub_pipeline.authority_files.repo_dir | regex_replace('\\/$', '') }}"
  become_user: "{{ application_user }}"

- name: Initialize datahub_pipeline_paths array
  set_fact:
    datahub_pipeline_fix_file_paths: {}

- name: Set absolute paths for fix file paths and store them in the new array
  set_fact:
    datahub_pipeline_fix_file_paths:
      "{{ datahub_pipeline_fix_file_paths 
        | combine({ item.shorthand: item.fix_file_path | regex_replace('^(.*)$', datahub_pipeline.dir + '/fixes/\\1') if not item.fix_file_path.startswith('/') else item.fix_file_path }) }}"
  with_items:
    - "{{ datahub_pipeline.institutions }}"

- name: Install perl
  apt:
    name:
      - perl

- name: Install apt packages
  apt:
    name:
      - sqlite3
      - cpanminus
      - libssl-dev
      - libxslt1-dev
      - libxml2-dev
      - make
      - build-essential
      - expat
      - libexpat1
      - libexpat1-dev
      - libcrypt-ssleay-perl
      - libsqlite3-dev
      - perl-modules
      - libdbd-sqlite3-perl
      - libcatmandu-perl
      - libcatmandu-aat-perl
      - libcatmandu-xml-perl
      - libxml-easy-perl
      - libcatmandu-oai-perl
      - libcatmandu-viaf-perl

- name: Install libmysqlclient-dev if Ubuntu
  apt:
    name:
      - libmysqlclient-dev
  when: ansible_facts.distribution == 'Ubuntu'

- name: Install default-libmysqlclient-dev if Debian
  apt:
    name:
      - default-libmysqlclient-dev
  when: ansible_facts.distribution == 'Debian'

# Install without tests
- name: Install additional pipeline dependencies
  cpanm:
    name: "{{ item }}"
  with_items:
    - Datahub::Factory
    - Datahub::Factory::Cmd
    - Datahub::Factory::Importer::EIZ
    - Datahub::Factory::Exporter
  become_user: "{{ application_user }}"
  environment:
    PERL5LIB: "{{ application_user_homedir }}/perl5/lib/perl5"
    PATH: "{{ application_user_homedir }}/perl5/bin:{{ ansible_env.PATH }}"

- name: Check if LIDO_1_0 directory exists
  stat:
    path: "{{ application_user_homedir }}/perl5/lib/perl5/Lido/XML/LIDO_1_0"
  register: lido_dir

- name: Check if backup already exists
  stat:
    path: "{{ application_user_homedir }}/perl5/lib/perl5/Lido/XML/LIDO_1_0/lido_v1_old.pm"
  register: lido_backup
  when: lido_dir.stat.exists

- name: Backup existing lido_v1.pm if present and no backup exists
  command: mv lido_v1.pm lido_v1_old.pm
  args:
    chdir: "{{ application_user_homedir }}/perl5/lib/perl5/Lido/XML/LIDO_1_0"
    removes: "{{ application_user_homedir }}/perl5/lib/perl5/Lido/XML/LIDO_1_0/lido_v1.pm"
  become_user: "{{ application_user }}"
  when:
    - lido_dir.stat.exists
    - not lido_backup.stat.exists

- name: Copy new lido_v1.pm if directory exists and no backup yet
  copy:
    src: lido_v1.pm
    dest: "{{ application_user_homedir }}/perl5/lib/perl5/Lido/XML/LIDO_1_0/lido_v1.pm"
    owner: "{{ application_user }}"
    group: "{{ application_user }}"
    mode: "0644"
  when:
    - lido_dir.stat.exists
    - not lido_backup.stat.exists

- name: Create datahub-pipeline folder
  file:
    path: "{{ datahub_pipeline.dir }}"
    state: directory
    owner: "{{ application_user }}"
    group: "{{ application_user }}"

- name: Create repo folders
  file: 
    path: "{{ item }}"
    state: directory
    recurse: true 
    owner: "{{ application_user }}"
    group: "{{ application_user }}"
  with_items:
    - "{{ datahub_pipeline.fixes.repo_dir }}"
    - "{{ datahub_pipeline.authority_files.repo_dir }}"

- name: Checkout datahub-pipeline repos
  git:
    repo: "{{ item.url }}"
    dest: "{{ item.dest }}/repo/"
    force: true
    version: "{{ item.branch }}"
  become_user: "{{ application_user }}"
  with_items:
    - url: "{{ datahub_pipeline.fixes.repo_url }}"
      dest: "{{ datahub_pipeline.fixes.repo_dir }}"
      branch: "{{ datahub_pipeline.fixes.branch }}"
    - url: "{{ datahub_pipeline.authority_files.repo_url }}"
      dest: "{{ datahub_pipeline.authority_files.repo_dir }}"
      branch: "{{ datahub_pipeline.authority_files.branch }}"

- name: Override vocabularies.sh
  template:
    src: vocabularies.sh
    dest: "{{ datahub_pipeline.fixes.repo_dir }}/repo/scripts/vocabularies.sh"
  become_user: "{{ application_user }}"

- name: Copy datahub.ini for datahub pipeline
  template:
    src: datahub.ini
    dest: "{{ datahub_pipeline.dir }}/datahub.ini"
  become_user: "{{ application_user }}"

- name: Link fixes and authority_files repos
  file:
    src: "{{ item.src }}/repo/"
    dest: "{{ item.dest }}"
    state: link
    owner: "{{ application_user }}"
    group: "{{ application_user }}"
  with_items:
    - src: "{{ datahub_pipeline.fixes.repo_dir }}"
      dest: "{{ datahub_pipeline.dir }}/fixes"
    - src: "{{ datahub_pipeline.authority_files.repo_dir }}"
      dest: "{{ datahub_pipeline.dir }}/authority_files"

- name: Provide additional crontab entry/entries to download IIIF manifest sqlite databas(es) from relevant imagehub(s)
  set_fact:
    cronjobs: "{{ cronjobs + [ item ] }}"
  with_items: "{{ datahub_pipeline.imagehub_cronjobs }}"

- name: Provide crontab entry for daily reload of datahub records (no output file, not verbose)
  set_fact:
    cronjobs: "{{ cronjobs + [ 'PERL5LIB=~' ~ application_user ~ '/perl5/bin/dhconveyor transport -p ' ~ datahub_pipeline.dir ~ '/datahub.ini -L1' ] }}"
  when: datahub_pipeline.output_file | length == 0 and not datahub_pipeline.output_verbose

- name: Provide crontab entry for daily reload of datahub records (no output file, verbose)
  set_fact:
    cronjobs: "{{ cronjobs + [ 'PERL5LIB=~' ~ application_user ~ '/perl5/bin/dhconveyor transport -p ' ~ datahub_pipeline.dir ~ '/datahub.ini -v -L1' ] }}"
  when: datahub_pipeline.output_file | length == 0 and datahub_pipeline.output_verbose

- name: Provide crontab entry for daily reload of datahub records (output file, not verbose)
  set_fact:
    cronjobs: "{{ cronjobs + [ 'PERL5LIB=~' ~ application_user ~ '/perl5/lib/perl5/ script -q -c \"~' ~ application_user ~ '/perl5/bin/dhconveyor transport -p ' ~ datahub_pipeline.dir ~ '/datahub.ini -L1\" > ' ~ datahub_pipeline.output_file ] }}"
  when: datahub_pipeline.output_file | length > 0 and not datahub_pipeline.output_verbose

- name: Provide crontab entry for daily reload of datahub records (output file, verbose)
  set_fact:
    cronjobs: "{{ cronjobs + [ 'PERL5LIB=~' ~ application_user ~ '/perl5/lib/perl5/ script -q -c \"~' ~ application_user ~ '/perl5/bin/dhconveyor transport -p ' ~ datahub_pipeline.dir ~ '/datahub.ini -v -L1\" > ' ~ datahub_pipeline.output_file ] }}"
  when: datahub_pipeline.output_file | length > 0 and datahub_pipeline.output_verbose
